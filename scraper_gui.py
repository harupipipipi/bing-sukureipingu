#!/usr/bin/env python3
"""
Streamlit GUI for Web Scraping Tool
ç°¡å˜ãªWebã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
"""

import streamlit as st
import pandas as pd
from fast_scraper import FastWebScraper
import asyncio
import json
import os
from datetime import datetime

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="çˆ†é€ŸWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸš€",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'scraping_results' not in st.session_state:
    st.session_state.scraping_results = None
if 'output_dir' not in st.session_state:
    st.session_state.output_dir = None

def main():
    # ã‚¿ã‚¤ãƒˆãƒ«
    st.title("ğŸš€ çˆ†é€ŸWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")
    st.markdown("Bingæ¤œç´¢ã‚’ä½¿ç”¨ã—ã¦ä¸Šä½5ä»¶ã®ã‚µã‚¤ãƒˆã‚’é«˜é€Ÿã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("ğŸ“‹ æ©Ÿèƒ½èª¬æ˜")
        st.markdown("""
        - **Bingæ¤œç´¢**: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ä¸Šä½5ä»¶ã‚’è‡ªå‹•å–å¾—
        - **ä¸¦åˆ—å‡¦ç†**: è¤‡æ•°ã‚µã‚¤ãƒˆã‚’åŒæ™‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        - **ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º**: ãƒšãƒ¼ã‚¸å†…å®¹ã‚’æ•´å½¢ã—ã¦å–å¾—
        - **ç”»åƒURLåé›†**: å…¨ç”»åƒã®URLã‚’ãƒªã‚¹ãƒˆåŒ–
        - **ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›**: .txtå½¢å¼ã§çµæœã‚’ä¿å­˜
        - **AIé€£æº**: JSONå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›
        """)
        
        st.header("ğŸ’¡ ä½¿ã„æ–¹")
        st.markdown("""
        1. æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›
        2. ã€Œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
        3. çµæœã‚’ç¢ºèªãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        """)
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ 
        search_query = st.text_input(
            "ğŸ” æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›",
            placeholder="ä¾‹: Python ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° å…¥é–€",
            help="Bingæ¤œç´¢ã§ä½¿ç”¨ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"
        )
        
        if st.button("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹", type="primary", disabled=not search_query):
            if search_query:
                with st.spinner("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œä¸­... (æœ€å¤§30ç§’)"):
                    try:
                        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
                        scraper = FastWebScraper()
                        
                        # URLå–å¾—
                        urls = scraper.search_bing(search_query, num_results=5)
                        
                        if urls:
                            # éåŒæœŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                            results = loop.run_until_complete(scraper.scrape_urls_async(urls))
                            loop.close()
                            
                            # çµæœã‚’ä¿å­˜
                            output_dir = scraper.save_results(search_query, results)
                            
                            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                            st.session_state.scraping_results = results
                            st.session_state.output_dir = output_dir
                            
                            st.success(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼ {len(results)}ä»¶ã®ã‚µã‚¤ãƒˆã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
                        else:
                            st.error("âŒ æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                            
                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    
    with col2:
        if st.session_state.output_dir:
            st.info(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: {st.session_state.output_dir}")
    
    # çµæœè¡¨ç¤º
    if st.session_state.scraping_results:
        st.header("ğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ")
        
        # ã‚¿ãƒ–ã§çµæœã‚’æ•´ç†
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹", "ğŸ–¼ï¸ ç”»åƒURL", "ğŸ“Š çµ±è¨ˆæƒ…å ±", "ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"])
        
        with tab1:
            st.subheader("å–å¾—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆå†…å®¹")
            for i, result in enumerate(st.session_state.scraping_results, 1):
                with st.expander(f"ã‚µã‚¤ãƒˆ {i}: {result['url']}", expanded=False):
                    st.text_area(
                        "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„",
                        value=result['content'][:3000],
                        height=300,
                        key=f"content_{i}"
                    )
                    if len(result['content']) > 3000:
                        st.info("â€» è¡¨ç¤ºã¯æœ€åˆã®3000æ–‡å­—ã®ã¿ã€‚å®Œå…¨ç‰ˆã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        
        with tab2:
            st.subheader("å–å¾—ã—ãŸç”»åƒURL")
            for i, result in enumerate(st.session_state.scraping_results, 1):
                if result['images']:
                    with st.expander(f"ã‚µã‚¤ãƒˆ {i}: {len(result['images'])}å€‹ã®ç”»åƒ", expanded=False):
                        for j, img_url in enumerate(result['images'][:10], 1):
                            st.text(f"{j}. {img_url}")
                        if len(result['images']) > 10:
                            st.info(f"â€» ä»–{len(result['images'])-10}å€‹ã®ç”»åƒURL")
        
        with tab3:
            st.subheader("çµ±è¨ˆæƒ…å ±")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§è¡¨ç¤º
            stats_data = []
            for i, result in enumerate(st.session_state.scraping_results, 1):
                stats_data.append({
                    'ã‚µã‚¤ãƒˆç•ªå·': i,
                    'URL': result['url'],
                    'ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°': len(result['content']),
                    'ç”»åƒæ•°': len(result['images']),
                    'å–å¾—æ™‚åˆ»': result['scraped_at']
                })
            
            df = pd.DataFrame(stats_data)
            st.dataframe(df, use_container_width=True)
            
            # ã‚µãƒãƒªãƒ¼
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ç·ã‚µã‚¤ãƒˆæ•°", len(stats_data))
            with col2:
                st.metric("ç·æ–‡å­—æ•°", f"{sum(d['ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°'] for d in stats_data):,}")
            with col3:
                st.metric("ç·ç”»åƒæ•°", sum(d['ç”»åƒæ•°'] for d in stats_data))
        
        with tab4:
            st.subheader("ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
            
            if st.session_state.output_dir and os.path.exists(st.session_state.output_dir):
                st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
                files = os.listdir(st.session_state.output_dir)
                
                for file in files:
                    file_path = os.path.join(st.session_state.output_dir, file)
                    if os.path.isfile(file_path):
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        st.download_button(
                            label=f"ğŸ“¥ {file}",
                            data=content,
                            file_name=file,
                            mime='text/plain' if file.endswith('.txt') else 'application/json'
                        )

if __name__ == "__main__":
    main()