#!/usr/bin/env python3
"""
é«˜é€ŸWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«
Bingæ¤œç´¢ã‚’ä½¿ç”¨ã—ã¦ä¸Šä½5ä»¶ã®ã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒURLã‚’æŠ½å‡ºã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
"""

import asyncio
import aiohttp
import requests
from bs4 import BeautifulSoup
import html2text
import json
import time
from datetime import datetime
import re
import os
from urllib.parse import urljoin, urlparse, quote
from typing import List, Dict, Tuple
import concurrent.futures
from functools import partial

class FastWebScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        self.html_converter = html2text.HTML2Text()
        self.html_converter.ignore_links = False
        self.html_converter.ignore_images = False
        self.html_converter.body_width = 0
        
    def search_bing(self, query: str, num_results: int = 5) -> List[str]:
        """Bingæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦ä¸Šä½ã®URLã‚’å–å¾—"""
        print(f"\nğŸ” Bingæ¤œç´¢å®Ÿè¡Œä¸­: '{query}'")
        
        # Bingæ¤œç´¢URLã‚’æ§‹ç¯‰
        search_url = f"https://www.bing.com/search?q={quote(query)}&count={num_results * 2}"
        
        try:
            response = requests.get(search_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # æ¤œç´¢çµæœã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            urls = []
            
            # é€šå¸¸ã®æ¤œç´¢çµæœã‚’å–å¾—
            for result in soup.find_all('li', class_='b_algo'):
                link = result.find('h2')
                if link and link.find('a'):
                    url = link.find('a').get('href')
                    if url and url.startswith('http'):
                        urls.append(url)
                        if len(urls) >= num_results:
                            break
            
            # ä»£æ›¿ã®æ¤œç´¢çµæœã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            if len(urls) < num_results:
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if href.startswith('http') and 'bing.com' not in href and 'microsoft.com' not in href:
                        if href not in urls:
                            urls.append(href)
                            if len(urls) >= num_results:
                                break
            
            print(f"âœ… {len(urls)}ä»¶ã®URLã‚’å–å¾—ã—ã¾ã—ãŸ")
            return urls[:num_results]
            
        except Exception as e:
            print(f"âŒ Bingæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    async def fetch_page_async(self, session: aiohttp.ClientSession, url: str) -> Tuple[str, str, List[str]]:
        """éåŒæœŸã§ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ç”»åƒURLã‚’æŠ½å‡º"""
        try:
            async with session.get(url, headers=self.headers, timeout=aiohttp.ClientTimeout(total=15)) as response:
                if response.status == 200:
                    html_content = await response.text()
                    
                    # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
                    soup = BeautifulSoup(html_content, 'lxml')
                    
                    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚¿ã‚°ã‚’å‰Šé™¤
                    for script in soup(["script", "style", "noscript"]):
                        script.decompose()
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
                    text_content = self.html_converter.handle(str(soup))
                    
                    # ç”»åƒURLã‚’æŠ½å‡º
                    image_urls = []
                    for img in soup.find_all('img'):
                        img_url = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                        if img_url:
                            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                            absolute_url = urljoin(url, img_url)
                            if absolute_url.startswith('http'):
                                image_urls.append(absolute_url)
                    
                    # og:imageãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰ã‚‚ç”»åƒã‚’å–å¾—
                    og_image = soup.find('meta', property='og:image')
                    if og_image and og_image.get('content'):
                        og_img_url = urljoin(url, og_image['content'])
                        if og_img_url not in image_urls:
                            image_urls.append(og_img_url)
                    
                    return url, text_content, image_urls
                else:
                    return url, f"Error: HTTP {response.status}", []
                    
        except asyncio.TimeoutError:
            return url, "Error: Timeout", []
        except Exception as e:
            return url, f"Error: {str(e)}", []
    
    async def scrape_urls_async(self, urls: List[str]) -> List[Dict]:
        """è¤‡æ•°ã®URLã‚’éåŒæœŸã§é«˜é€Ÿã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        print(f"\nâš¡ {len(urls)}ä»¶ã®ã‚µã‚¤ãƒˆã‚’ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        
        results = []
        async with aiohttp.ClientSession() as session:
            tasks = [self.fetch_page_async(session, url) for url in urls]
            responses = await asyncio.gather(*tasks)
            
            for i, (url, content, images) in enumerate(responses, 1):
                print(f"  [{i}/{len(urls)}] âœ… {urlparse(url).netloc}")
                results.append({
                    'url': url,
                    'content': content,
                    'images': images,
                    'scraped_at': datetime.now().isoformat()
                })
        
        return results
    
    def save_results(self, query: str, results: List[Dict]):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_query = re.sub(r'[^\w\s-]', '', query)[:50]
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        output_dir = f"scraping_results_{safe_query}_{timestamp}"
        os.makedirs(output_dir, exist_ok=True)
        
        # ãƒ¡ã‚¤ãƒ³çµæœãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå…¨ã‚µã‚¤ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
        main_file = os.path.join(output_dir, "all_content.txt")
        with open(main_file, 'w', encoding='utf-8') as f:
            f.write(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ\n")
            f.write(f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {query}\n")
            f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"å–å¾—ã‚µã‚¤ãƒˆæ•°: {len(results)}\n")
            f.write("="*80 + "\n\n")
            
            for i, result in enumerate(results, 1):
                f.write(f"\n{'='*80}\n")
                f.write(f"ã‚µã‚¤ãƒˆ {i}: {result['url']}\n")
                f.write(f"å–å¾—æ—¥æ™‚: {result['scraped_at']}\n")
                f.write(f"ç”»åƒæ•°: {len(result['images'])}\n")
                f.write("-"*80 + "\n\n")
                f.write(result['content'][:50000])  # æœ€å¤§50000æ–‡å­—ã¾ã§
                if len(result['content']) > 50000:
                    f.write("\n\n[... ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒé•·ã™ãã‚‹ãŸã‚çœç•¥ ...]\n")
                f.write("\n\n")
        
        # å€‹åˆ¥ã‚µã‚¤ãƒˆã”ã¨ã®ãƒ•ã‚¡ã‚¤ãƒ«
        for i, result in enumerate(results, 1):
            site_file = os.path.join(output_dir, f"site_{i}_content.txt")
            with open(site_file, 'w', encoding='utf-8') as f:
                f.write(f"URL: {result['url']}\n")
                f.write(f"å–å¾—æ—¥æ™‚: {result['scraped_at']}\n")
                f.write("="*80 + "\n\n")
                f.write(result['content'])
        
        # ç”»åƒURLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
        images_file = os.path.join(output_dir, "all_image_urls.txt")
        with open(images_file, 'w', encoding='utf-8') as f:
            f.write(f"ç”»åƒURLä¸€è¦§\n")
            f.write(f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {query}\n")
            f.write("="*80 + "\n\n")
            
            for i, result in enumerate(results, 1):
                f.write(f"\nã‚µã‚¤ãƒˆ {i}: {result['url']}\n")
                f.write(f"ç”»åƒæ•°: {len(result['images'])}\n")
                f.write("-"*40 + "\n")
                
                if result['images']:
                    for j, img_url in enumerate(result['images'], 1):
                        f.write(f"{j}. {img_url}\n")
                else:
                    f.write("ç”»åƒãªã—\n")
                f.write("\n")
        
        # AIç”¨ã®JSONå½¢å¼ã§ã‚‚ä¿å­˜
        ai_data_file = os.path.join(output_dir, "ai_data.json")
        ai_data = {
            'query': query,
            'scraped_at': datetime.now().isoformat(),
            'results': []
        }
        
        for result in results:
            ai_data['results'].append({
                'url': result['url'],
                'content_preview': result['content'][:1000],  # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã¿
                'full_content_length': len(result['content']),
                'image_urls': result['images'][:20],  # æœ€å¤§20å€‹ã®ç”»åƒURL
                'total_images': len(result['images']),
                'scraped_at': result['scraped_at']
            })
        
        with open(ai_data_file, 'w', encoding='utf-8') as f:
            json.dump(ai_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_dir}/")
        print(f"  - all_content.txt: å…¨ã‚µã‚¤ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆ")
        print(f"  - site_*_content.txt: å€‹åˆ¥ã‚µã‚¤ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆ")
        print(f"  - all_image_urls.txt: å…¨ç”»åƒURLãƒªã‚¹ãƒˆ")
        print(f"  - ai_data.json: AIå‡¦ç†ç”¨ãƒ‡ãƒ¼ã‚¿")
        
        return output_dir
    
    def scrape(self, query: str):
        """ãƒ¡ã‚¤ãƒ³ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†"""
        start_time = time.time()
        
        print("\n" + "="*80)
        print("ğŸš€ é«˜é€ŸWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        print("="*80)
        
        # Bingæ¤œç´¢ã§URLã‚’å–å¾—
        urls = self.search_bing(query, num_results=5)
        
        if not urls:
            print("âŒ æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None
        
        print(f"\nå–å¾—ã—ãŸURL:")
        for i, url in enumerate(urls, 1):
            print(f"  {i}. {url}")
        
        # éåŒæœŸã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        results = loop.run_until_complete(self.scrape_urls_async(urls))
        loop.close()
        
        # çµæœã‚’ä¿å­˜
        output_dir = self.save_results(query, results)
        
        elapsed_time = time.time() - start_time
        print(f"\nâ±ï¸ å‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’")
        print(f"âœ¨ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼")
        
        return output_dir

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("="*80)
    print("ğŸ”¥ çˆ†é€ŸWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")
    print("="*80)
    print("\næ©Ÿèƒ½:")
    print("  - Bingæ¤œç´¢ã§ä¸Šä½5ä»¶ã®ã‚µã‚¤ãƒˆã‚’è‡ªå‹•å–å¾—")
    print("  - ä¸¦åˆ—å‡¦ç†ã§é«˜é€Ÿã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
    print("  - ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ç”»åƒURLã‚’æŠ½å‡º")
    print("  - çµæœã‚’æ•´ç†ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜")
    print("  - AIå‡¦ç†ç”¨ã®JSONå½¢å¼ã§ã‚‚å‡ºåŠ›")
    print("\n")
    
    # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›
    query = input("ğŸ” æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
    
    if not query:
        print("âŒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    scraper = FastWebScraper()
    output_dir = scraper.scrape(query)
    
    if output_dir:
        print(f"\nğŸ“Š çµæœã‚µãƒãƒªãƒ¼:")
        print(f"  æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {query}")
        print(f"  å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
        print(f"\nğŸ’¡ ãƒ’ãƒ³ãƒˆ: AIå‡¦ç†ã«ã¯ ai_data.json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    main()