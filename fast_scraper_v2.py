#!/usr/bin/env python3
"""
é«˜é€ŸWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« v2
ç›´æ¥URLã‚’æŒ‡å®šã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã€ã¾ãŸã¯Googleæ¤œç´¢APIã‚’ä½¿ç”¨
"""

import asyncio
import aiohttp
import requests
from bs4 import BeautifulSoup
import html2text
import json
import time
from datetime import datetime
import re
import os
from urllib.parse import urljoin, urlparse, quote, unquote
from typing import List, Dict, Tuple

class FastWebScraperV2:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        self.html_converter = html2text.HTML2Text()
        self.html_converter.ignore_links = False
        self.html_converter.ignore_images = False
        self.html_converter.body_width = 0
        
    def search_google_custom(self, query: str, num_results: int = 5) -> List[str]:
        """Googleæ¤œç´¢ã®ä»£æ›¿å®Ÿè£…ï¼ˆDuckDuckGoã‚’ä½¿ç”¨ï¼‰"""
        print(f"\nğŸ” Webæ¤œç´¢å®Ÿè¡Œä¸­: '{query}'")
        
        # DuckDuckGo HTMLç‰ˆã‚’ä½¿ç”¨
        search_url = f"https://html.duckduckgo.com/html/?q={quote(query)}"
        
        try:
            response = requests.get(search_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            urls = []
            # DuckDuckGoã®æ¤œç´¢çµæœã‚’å–å¾—
            for result in soup.find_all('a', class_='result__a'):
                url = result.get('href')
                if url and url.startswith('http'):
                    urls.append(url)
                    if len(urls) >= num_results:
                        break
            
            # ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            if len(urls) < num_results:
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if href.startswith('http') and 'duckduckgo.com' not in href:
                        if href not in urls:
                            urls.append(href)
                            if len(urls) >= num_results:
                                break
            
            print(f"âœ… {len(urls)}ä»¶ã®URLã‚’å–å¾—ã—ã¾ã—ãŸ")
            return urls[:num_results]
            
        except Exception as e:
            print(f"âš ï¸ Webæ¤œç´¢ã§å•é¡Œç™ºç”Ÿ: {str(e)}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«URLã‚’æä¾›
            print("ğŸ“Œ ã‚µãƒ³ãƒ—ãƒ«URLã‚’ä½¿ç”¨ã—ã¾ã™")
            return self.get_sample_urls()
    
    def get_sample_urls(self) -> List[str]:
        """ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«URL"""
        return [
            "https://www.python.org/",
            "https://docs.python.org/3/tutorial/",
            "https://realpython.com/",
            "https://www.w3schools.com/python/",
            "https://github.com/python/cpython"
        ]
    
    async def fetch_page_async(self, session: aiohttp.ClientSession, url: str) -> Tuple[str, str, List[str]]:
        """éåŒæœŸã§ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ç”»åƒURLã‚’æŠ½å‡º"""
        try:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’çŸ­ãè¨­å®š
            timeout = aiohttp.ClientTimeout(total=10)
            async with session.get(url, headers=self.headers, timeout=timeout, ssl=False) as response:
                if response.status == 200:
                    html_content = await response.text()
                    
                    # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
                    soup = BeautifulSoup(html_content, 'lxml')
                    
                    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚¿ã‚°ã‚’å‰Šé™¤
                    for script in soup(["script", "style", "noscript"]):
                        script.decompose()
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                    title = soup.find('title')
                    title_text = title.text if title else "No Title"
                    
                    # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
                    meta_desc = soup.find('meta', attrs={'name': 'description'})
                    description = meta_desc.get('content', '') if meta_desc else ''
                    
                    # æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                    # mainã‚¿ã‚°ã€articleã‚¿ã‚°ã€ã¾ãŸã¯bodyã‚¿ã‚°ã‹ã‚‰å–å¾—
                    main_content = soup.find('main') or soup.find('article') or soup.find('body')
                    if main_content:
                        text_content = self.html_converter.handle(str(main_content))
                    else:
                        text_content = self.html_converter.handle(str(soup))
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆã®å‰ã«ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜ã‚’è¿½åŠ 
                    full_content = f"# {title_text}\n\n"
                    if description:
                        full_content += f"**èª¬æ˜**: {description}\n\n"
                    full_content += text_content
                    
                    # ç”»åƒURLã‚’æŠ½å‡º
                    image_urls = []
                    for img in soup.find_all('img'):
                        img_url = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                        if img_url:
                            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                            absolute_url = urljoin(url, img_url)
                            if absolute_url.startswith('http'):
                                # ç”»åƒã®alt textã‚‚å–å¾—
                                alt_text = img.get('alt', '')
                                image_info = {'url': absolute_url, 'alt': alt_text}
                                image_urls.append(absolute_url)
                    
                    # og:imageãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰ã‚‚ç”»åƒã‚’å–å¾—
                    og_image = soup.find('meta', property='og:image')
                    if og_image and og_image.get('content'):
                        og_img_url = urljoin(url, og_image['content'])
                        if og_img_url not in image_urls:
                            image_urls.append(og_img_url)
                    
                    return url, full_content, image_urls
                else:
                    return url, f"Error: HTTP {response.status}", []
                    
        except asyncio.TimeoutError:
            return url, "Error: Timeout (10ç§’)", []
        except Exception as e:
            return url, f"Error: {str(e)}", []
    
    async def scrape_urls_async(self, urls: List[str]) -> List[Dict]:
        """è¤‡æ•°ã®URLã‚’éåŒæœŸã§é«˜é€Ÿã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        print(f"\nâš¡ {len(urls)}ä»¶ã®ã‚µã‚¤ãƒˆã‚’ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        
        results = []
        # ã‚³ãƒã‚¯ã‚¿ãƒ¼ã®è¨­å®šã‚’èª¿æ•´
        connector = aiohttp.TCPConnector(limit=5, force_close=True)
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = [self.fetch_page_async(session, url) for url in urls]
            responses = await asyncio.gather(*tasks)
            
            for i, (url, content, images) in enumerate(responses, 1):
                status = "âœ…" if not content.startswith("Error:") else "âš ï¸"
                print(f"  [{i}/{len(urls)}] {status} {urlparse(url).netloc}")
                results.append({
                    'url': url,
                    'content': content,
                    'images': images,
                    'scraped_at': datetime.now().isoformat()
                })
        
        return results
    
    def save_results(self, query: str, results: List[Dict]):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_query = re.sub(r'[^\w\s-]', '', query)[:50]
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        output_dir = f"scraping_results_{safe_query}_{timestamp}"
        os.makedirs(output_dir, exist_ok=True)
        
        # ãƒ¡ã‚¤ãƒ³çµæœãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå…¨ã‚µã‚¤ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
        main_file = os.path.join(output_dir, "all_content.txt")
        with open(main_file, 'w', encoding='utf-8') as f:
            f.write(f"ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ\n")
            f.write(f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {query}\n")
            f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"å–å¾—ã‚µã‚¤ãƒˆæ•°: {len(results)}\n")
            f.write("="*80 + "\n\n")
            
            success_count = sum(1 for r in results if not r['content'].startswith("Error:"))
            f.write(f"æˆåŠŸ: {success_count}ä»¶ / å¤±æ•—: {len(results) - success_count}ä»¶\n")
            f.write("="*80 + "\n\n")
            
            for i, result in enumerate(results, 1):
                f.write(f"\n{'='*80}\n")
                f.write(f"ã‚µã‚¤ãƒˆ {i}: {result['url']}\n")
                f.write(f"å–å¾—æ—¥æ™‚: {result['scraped_at']}\n")
                f.write(f"ç”»åƒæ•°: {len(result['images'])}\n")
                f.write("-"*80 + "\n\n")
                
                if result['content'].startswith("Error:"):
                    f.write(f"âš ï¸ {result['content']}\n")
                else:
                    f.write(result['content'][:50000])  # æœ€å¤§50000æ–‡å­—ã¾ã§
                    if len(result['content']) > 50000:
                        f.write("\n\n[... ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒé•·ã™ãã‚‹ãŸã‚çœç•¥ ...]\n")
                f.write("\n\n")
        
        # å€‹åˆ¥ã‚µã‚¤ãƒˆã”ã¨ã®ãƒ•ã‚¡ã‚¤ãƒ«
        for i, result in enumerate(results, 1):
            if not result['content'].startswith("Error:"):
                site_file = os.path.join(output_dir, f"site_{i}_content.txt")
                with open(site_file, 'w', encoding='utf-8') as f:
                    f.write(f"URL: {result['url']}\n")
                    f.write(f"å–å¾—æ—¥æ™‚: {result['scraped_at']}\n")
                    f.write("="*80 + "\n\n")
                    f.write(result['content'])
        
        # ç”»åƒURLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
        images_file = os.path.join(output_dir, "all_image_urls.txt")
        with open(images_file, 'w', encoding='utf-8') as f:
            f.write(f"ğŸ–¼ï¸ ç”»åƒURLä¸€è¦§\n")
            f.write(f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {query}\n")
            f.write("="*80 + "\n\n")
            
            total_images = 0
            for i, result in enumerate(results, 1):
                if result['images']:
                    f.write(f"\nã‚µã‚¤ãƒˆ {i}: {result['url']}\n")
                    f.write(f"ç”»åƒæ•°: {len(result['images'])}\n")
                    f.write("-"*40 + "\n")
                    
                    for j, img_url in enumerate(result['images'], 1):
                        f.write(f"{j}. {img_url}\n")
                    total_images += len(result['images'])
                    f.write("\n")
            
            f.write(f"\nç·ç”»åƒæ•°: {total_images}\n")
        
        # AIç”¨ã®JSONå½¢å¼ã§ã‚‚ä¿å­˜
        ai_data_file = os.path.join(output_dir, "ai_data.json")
        ai_data = {
            'query': query,
            'scraped_at': datetime.now().isoformat(),
            'total_sites': len(results),
            'successful_sites': sum(1 for r in results if not r['content'].startswith("Error:")),
            'results': []
        }
        
        for result in results:
            if not result['content'].startswith("Error:"):
                ai_data['results'].append({
                    'url': result['url'],
                    'content_preview': result['content'][:1000],  # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã¿
                    'full_content_length': len(result['content']),
                    'image_urls': result['images'][:20],  # æœ€å¤§20å€‹ã®ç”»åƒURL
                    'total_images': len(result['images']),
                    'scraped_at': result['scraped_at']
                })
        
        with open(ai_data_file, 'w', encoding='utf-8') as f:
            json.dump(ai_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_dir}/")
        print(f"  - all_content.txt: å…¨ã‚µã‚¤ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆ")
        print(f"  - site_*_content.txt: å€‹åˆ¥ã‚µã‚¤ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆ")
        print(f"  - all_image_urls.txt: å…¨ç”»åƒURLãƒªã‚¹ãƒˆ")
        print(f"  - ai_data.json: AIå‡¦ç†ç”¨ãƒ‡ãƒ¼ã‚¿")
        
        return output_dir
    
    def scrape(self, query: str = None, urls: List[str] = None):
        """ãƒ¡ã‚¤ãƒ³ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†"""
        start_time = time.time()
        
        print("\n" + "="*80)
        print("ğŸš€ é«˜é€ŸWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ v2")
        print("="*80)
        
        # URLãƒªã‚¹ãƒˆã®å–å¾—
        if urls:
            # ç›´æ¥URLãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ
            print(f"ğŸ“Œ æŒ‡å®šã•ã‚ŒãŸ{len(urls)}ä»¶ã®URLã‚’ä½¿ç”¨")
        elif query:
            # æ¤œç´¢ã‚¯ã‚¨ãƒªãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ
            urls = self.search_google_custom(query, num_results=5)
            if not urls:
                print("âŒ æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return None
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚µãƒ³ãƒ—ãƒ«URLã‚’ä½¿ç”¨
            print("ğŸ“Œ ã‚µãƒ³ãƒ—ãƒ«URLã‚’ä½¿ç”¨ã—ã¾ã™")
            urls = self.get_sample_urls()
            query = "Python Programming Sample"
        
        print(f"\nå–å¾—ã™ã‚‹URL:")
        for i, url in enumerate(urls, 1):
            print(f"  {i}. {url}")
        
        # éåŒæœŸã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        results = loop.run_until_complete(self.scrape_urls_async(urls))
        loop.close()
        
        # çµæœã‚’ä¿å­˜
        output_dir = self.save_results(query or "Direct URLs", results)
        
        elapsed_time = time.time() - start_time
        print(f"\nâ±ï¸ å‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’")
        print(f"âœ¨ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼")
        
        return output_dir

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("="*80)
    print("ğŸ”¥ çˆ†é€ŸWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« v2")
    print("="*80)
    print("\næ©Ÿèƒ½:")
    print("  - Webæ¤œç´¢ã¾ãŸã¯ç›´æ¥URLæŒ‡å®šã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
    print("  - ä¸¦åˆ—å‡¦ç†ã§é«˜é€Ÿã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
    print("  - ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ç”»åƒURLã‚’æŠ½å‡º")
    print("  - çµæœã‚’æ•´ç†ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜")
    print("  - AIå‡¦ç†ç”¨ã®JSONå½¢å¼ã§ã‚‚å‡ºåŠ›")
    print("\n")
    
    # ä½¿ç”¨æ–¹æ³•ã‚’é¸æŠ
    print("ä½¿ç”¨æ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    print("1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
    print("2. URLç›´æ¥æŒ‡å®šã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
    print("3. ã‚µãƒ³ãƒ—ãƒ«URLã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    
    choice = input("\né¸æŠ (1/2/3): ").strip()
    
    scraper = FastWebScraperV2()
    
    if choice == "1":
        # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›
        query = input("ğŸ” æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
        if not query:
            print("âŒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        output_dir = scraper.scrape(query=query)
    
    elif choice == "2":
        # URLç›´æ¥å…¥åŠ›
        print("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°ã®å ´åˆã¯æ”¹è¡Œã§åŒºåˆ‡ã‚‹ã€ç©ºè¡Œã§çµ‚äº†ï¼‰:")
        urls = []
        while True:
            url = input().strip()
            if not url:
                break
            if url.startswith("http"):
                urls.append(url)
        
        if not urls:
            print("âŒ URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        output_dir = scraper.scrape(urls=urls)
    
    else:
        # ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
        print("ğŸ“Œ ã‚µãƒ³ãƒ—ãƒ«URLã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã—ã¾ã™...")
        output_dir = scraper.scrape()
    
    if output_dir:
        print(f"\nğŸ“Š çµæœã‚µãƒãƒªãƒ¼:")
        print(f"  å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
        print(f"\nğŸ’¡ ãƒ’ãƒ³ãƒˆ: AIå‡¦ç†ã«ã¯ ai_data.json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    main()